{% extends "tutorial.html" %}
{% load mixin from templatefilters %}

{% block pagebreadcrumb %}{{ tut.title }}{% endblock %}

{% block head %}

<style>
    #dry, #wet{
        width:200px;
        height:75px;
        margin: 50px;
        color: #333;
        border-radius: 15px;
        text-align: center;
        font-size: 25px;
        font-family: sans-serif;
        padding-top:25px;
        display: inline-block;
  	-webkit-user-select: none;
    }
    #dry{
        background-color: #e88;
    }
    #wet{
        background-color: #88e;
    }
    
    #dry:hover, #wet:hover{
        height:98px;
        cursor:pointer;
    }
</style>

{% endblock %}

{% block iscompatible %}
  return !!(window.AudioContext || window.webkitAudioContext);
{% endblock %}

{% block html5badge %}
<!-- Your HTML5 badge (tech class icons used in the article) goes here -->
{% endblock %}

{% block share_image %}
<!--<meta itemprop="image" content="images/your_social_sharing_img.png">-->
{% endblock %}


{% block content %}

<!-- Uncomment if this is a bleeding edge feature
  <p>{% include "warning.html" %}</p>
-->

<h2 id="toc-introduction">Introduction</h2>

<p><a href="http://www.chrome.com/racer">Racer</a> Racer is a multi-player, multi-device Chrome Experiment. A retro-style slot car game played across screens. On phones or tablets, Android or iOS. Anyone can join. No apps. No downloads. Just the mobile web.</p>

<p>Plan8 together with our friends at 14islands created the dynamic music and sound experience based on an original composition by Giorgio Moroder. Racer features responsive engine sounds, race sound effects, but more importantly a dynamic music mix that distributes itself over several devices as racers join. It’s a multi-speaker installation composed of smartphones.</p>

<p>Connecting multiple devices together was something we had been fooling around with for some time. We had done music experiments where sound would split up on different devices or jump between devices, so we were eager to apply those ideas to Racer.</p>

<p>More specifically we wanted to test if we could build up the music track across the devices as more and more people joined the game—starting with drums and bass, then adding guitar and synths and so on. We did some music demos and dived into coding. The multi speaker effect was really rewarding. We didn’t have all the syncing right at this point, but when we heard the layers of sound spread out over the devices we knew we were onto something good.</p>

<h2 id="toc-topic">Creating the sounds</h2>

<p>Google Creative Lab had outlined a creative direction for the sound and music. We wanted to use analogue synthesizers to create the sound effects rather than recording the real sounds or resorting to sound libraries. We also knew the output speaker would, in most cases, be a tiny phone or tablet speaker so the sounds had to be limited in frequency spectrum to avoid the speakers from distorting. This proved to be quite a challenge. When we received the first music drafts from Giorgio it was a relief because his composition worked perfectly with the sounds we had created.</p>

<p>Since we wanted to be able to use the native nodes and our own custom effects in such a transparent way as possible, we decided that we needed to create a wrapper format that could  achieve this. The native nodes in Web Audio uses its connect method to link nodes together, so we needed to emulate this behaviour. This is what the basic idea looks like:</p>

<pre class="prettyprint">
var MyCustomNode = function(){
    this.input = audioContext.createGainNode();
    var output = audioContext.createGainNode();

    this.connect = function(target){
       output.connect(target);
    };
};
</pre>

<p>With this pattern we’re really close to the native nodes. Let’s see how this would be used.</p>

<pre class="prettyprint">
//create a couple of native nodes and our custom node
var gain = audioContext.createGainNode(),
    customNode = new MyCustomNode(),
    anotherGain = audioContext.createGainNode();

//connect our custom node to the native nodes and send to the output
gain.connect(customNode.input);
customNode.connect(anotherGain);
anotherGain.connect(audioContext.destination);
</pre>

<img src="/static/images/tutorials/casestudies/jamwithchrome-audio/skeleton.png" alt="Routing the custom node"/>

<p>The only difference between our custom node and a native one is that we have to connect to the custom nodes input property. I’m sure there are ways to circumvent this, but this was close enough for our purposes. This pattern can be further developed to simulate the disconnect methods of native AudioNodes, as well as accommodating user defined inputs/outputs when connecting and so on. Have a look at the <a href="https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html#AudioNode-section">specification</a> to see what the native nodes can do.</p>

<p>Now that we had our basic pattern for creating custom effects, the next step were to actually give the custom node some custom behaviour. Let’s have a look at a slapback delay node.</p>

<h2 id="toc-topic">Engine sound</h2>

<p>The greatest challenge in programming the sounds was to find the best engine sound and sculpt its behavior. The race track resembled an F1 or Nascar track, so the cars had to feel fast and explosive. At the same time the cars were really small so a big engine sound wouldn’t really connect the sound to the visuals. We couldn’t have a beefy roaring engine playing in the mobile speaker anyway so we had to figure out something else.</p>

<pre class="prettyprint">
var SlapbackDelayNode = function(){
    //create the nodes we’ll use
    this.input = audioContext.createGainNode();
    var output = audioContext.createGainNode(),
        delay = audioContext.createDelayNode(),
        feedback = audioContext.createGainNode(),
        wetLevel = audioContext.createGainNode();

    //set some decent values
    delay.delayTime.value = 0.15; //150 ms delay
    feedback.gain.value = 0.25;
    wetLevel.gain.value = 0.25;

    //set up the routing
    this.input.connect(delay);
    this.input.connect(output);
    delay.connect(feedback);
    delay.connect(wetLevel);
    feedback.connect(delay);
    wetLevel.connect(output);

    this.connect = function(target){
       output.connect(target);
    };
};
</pre>

<img src="/static/images/tutorials/casestudies/jamwithchrome-audio/slapback.png" alt="Internal routing of the slapback node"/>

<p>As some of you might already have realised, this delay could be used with bigger delay times too, and thus become a regular mono delay with feedback. Here's an example using this delay to let you hear what it sounds like.</p> 

<h3 id="toc-topic-subtopic">Mouse down to produce sound</h3>
<div id="dry">Dry</div>
<div id="wet">Wet</div>

<p>There is much that can be done with this simple effect. It’s the base of other effects, like chorus and phasers, but it could also be further developed in its own right by, for example, inserting other effects that only affect the delayed signal.</p>

<h2 id="toc-topic">Routing audio</h2>

<p>When working with different instruments and musical parts in professional audio applications it’s essential to have a flexible routing system that allows you to mix and modulate the sounds in effective ways. In JAM with Chrome we have developed an audio bus system, similar to the ones found in physical mixing boards. This allows us to hook up all instruments that need a reverb effect to a common bus, or channel, and then add the reverb to that bus instead of adding a reverb to each separate instrument. This is a major optimization and it’s quite recommended to do something similar as soon as you start doing more complex applications.</p>

<img src="/static/images/tutorials/casestudies/jamwithchrome-audio/busrouting.png" alt="Routing of the AudioBus"/>

<p>Luckily this is really easy to achieve in Web Audio. We can basically use the skeleton we defined for the effects and use it in the same way.</p>

<pre class="prettyprint">
var AudioBus = function(){
    this.input = audioContext.createGainNode();
    var output = audioContext.createGainNode();

    //create effect nodes (Convolver and Equalizer are other custom effects from the library presented at the end of the article)
    var delay = new SlapbackDelayNode(),
        convolver = new tuna.Convolver(),
        equalizer = new tuna.Equalizer();

    //route ‘em
    //equalizer -> delay -> convolver
    this.input.connect(equalizer);
    equalizer.connect(delay.input);
    delay.connect(convolver);
    convolver.connect(output);

    this.connect = function(target){
       output.connect(target);
    };
};
</pre>

<p>This would be used like this:</p>

<pre class="prettyprint">
//create some native oscillators and our custom audio bus
var bus = new AudioBus(),
    instrument1 = audioContext.createOscillator(),
    instrument2 = audioContext.createOscillator(),
    instrument3 = audioContext.createOscillator();

//connect our instruments to the same bus
instrument1.connect(bus.input);
instrument2.connect(bus.input);
instrument3.connect(bus.input);
bus.connect(audioContext.destination);
</pre>

<p>And voila, we’ve applied delay, equalization and reverb (which is a rather expensive effect, performance wise) at half the cost as if we’d applied the effects to each separate instrument. If we wanted to add some extra spice to the bus, we could add two new gain nodes - preGain and postGain - which would allow us to turn off or fade the sounds in a bus in two different ways. The preGain is put before the effects, and postGain is put in the end of the chain. If we then fade the preGain the effects will still resonate after the gain has hit bottom, but if we fade postGain all sound will be muted at the same time.</p>

<h2 id="toc-topic">Where to from here?</h2>

<p>These methods I’ve described here can, and should, be further developed. Things like the input and output of the custom nodes, and the connect methods, could/should be implemented using prototype based inheritance. Buses should be able to create effects dynamically by being passed a list of effects. </p>

<p>To celebrate the release of JAM with Chrome we’ve decided to make our <a href=" https://github.com/Dinahmoe/tuna">framework of effects open source</a>. If this brief introduction has tickled your fancy, please have a look and feel free to contribute. There’s a discussion going on <a href="https://github.com/h5bp/lazyweb-requests/issues/82">here</a> regarding standardizing a format for custom Web Audio entites. Get involved!</p>

<script>
        window.onload = function(){
            
            //make sure we can play the demo
            if(!window.webkitAudioContext){
                document.getElementById("message").innerHTML = "Oops, your browser doesn't support this demo! Please try the latest Google Chrome.";
                return;
            } else{
                if(!window.Audio){
                    document.getElementById("message").innerHTML = "Oops, your browser doesn't support this demo! Please try the latest Google Chrome.";
                    return;
                }
                var a = new Audio(), filename;
                if(!!(a.canPlayType && a.canPlayType('audio/ogg; codecs="vorbis"').replace(/no/, ''))){
                   filename = "/static/audio/guitar_bright_hard_4_2.ogg";
                }
                else if(!!(a.canPlayType && a.canPlayType('audio/mp4; codecs="mp4a.40.2"').replace(/no/, ''))){
				   filename = "/static/audio/guitar_bright_hard_4_2.aac";
                }
                else {
			      document.getElementById("message").innerHTML = "Oops, your browser doesn't support this demo! Please try the latest Google Chrome.";
			      return;
			    }
            }
            
            var audioContext = new webkitAudioContext();
            
            //our custom node
            var SlapbackDelayNode = function(){
                //create the nodes we’ll use
                this.input = audioContext.createGainNode();
                var output = audioContext.createGainNode(),
                    delay = audioContext.createDelayNode(),
                    feedback = audioContext.createGainNode(),
                    wetLevel = audioContext.createGainNode();

                //set some decent values
                delay.delayTime.value = 0.15; //150 ms delay
                feedback.gain.value = 0.25;
                wetLevel.gain.value = 0.25;

                //set up the routing
                this.input.connect(delay);
                this.input.connect(output);
                delay.connect(feedback);
                delay.connect(wetLevel);
                feedback.connect(delay);
                wetLevel.connect(output);

                this.connect = function(target){
                   output.connect(target);
                };
            };
            
            var dryButton = document.getElementById("dry"),
                wetButton = document.getElementById("wet"),
                buffer,
                bufferSourceDry,
                bufferSourceWet,
                delay = new SlapbackDelayNode();
                
            //route delay
            delay.connect(audioContext.destination);
            
            //event callbacks
            function playDry(){
                bufferSourceDry = audioContext.createBufferSource();
                bufferSourceDry.buffer = buffer;
                bufferSourceDry.connect(audioContext.destination);
                bufferSourceDry.noteOn(audioContext.currentTime);
            }
            function playWet(){
                bufferSourceWet = audioContext.createBufferSource();
                bufferSourceWet.buffer = buffer;
                bufferSourceWet.connect(delay.input);
                bufferSourceWet.noteOn(audioContext.currentTime);
            }
            function stopDry(){
                bufferSourceDry.noteOff(audioContext.currentTime);
                bufferSourceDry.disconnect();
            }
            function stopWet(){
                bufferSourceWet.noteOff(audioContext.currentTime);
                bufferSourceWet.disconnect();
            }
            
                
            function setupListeners(){
                dryButton.addEventListener("mousedown", playDry);
                wetButton.addEventListener("mousedown", playWet);
                dryButton.addEventListener("mouseup", stopDry);
                wetButton.addEventListener("mouseup", stopWet);
            }    
                
            //load the guitar sound
            var request = new XMLHttpRequest();
            request.open("GET", filename, true);
            request.responseType = "arraybuffer";
            request.onload = function() {
                buffer = audioContext.createBuffer(request.response, false);
                setupListeners();
            };
            request.send();
        };
    </script>

{% endblock %}
