{% extends "tutorial.html" %}
{% load mixin from templatefilters %}

{% block pagebreadcrumb %}{{ tut.title }}{% endblock %}

{% block head %}

<style>
    #dry, #wet{
        width:200px;
        height:75px;
        margin: 50px;
        color: #333;
        border-radius: 15px;
        text-align: center;
        font-size: 25px;
        font-family: sans-serif;
        padding-top:25px;
        display: inline-block;
		-webkit-user-select: none;
    }
    #dry{
        background-color: #e88;
    }
    #wet{
        background-color: #88e;
    }
    
    #dry:hover, #wet:hover{
        height:98px;
        cursor:pointer;
    }
</style>

{% endblock %}

{% block iscompatible %}
  return !!(window.AudioContext || window.webkitAudioContext);
{% endblock %}

{% block html5badge %}
<!-- Your HTML5 badge (tech class icons used in the article) goes here -->
{% endblock %}

{% block share_image %}
<!--<meta itemprop="image" content="images/your_social_sharing_img.png">-->
{% endblock %}


{% block content %}

<!-- Uncomment if this is a bleeding edge feature
  <p>{% include "warning.html" %}</p>
-->

<h2 id="toc-introduction">Introduction</h2>

<p><a href="http://www.jamwithchrome.com">JAM with Chrome</a> is a web based musical project created by Google. JAM with Chrome lets people from all over the world form a band and jam in real time inside the browser. We at <a href="http://www.dinahmoe.com/">DinahMoe</a> had the great pleasure to be a part of this project. Our role was to produce music for the application, and design and develop the music component. The development consisted of three main areas: a "<a href="http://en.wikipedia.org/wiki/Music_Workstation">music workstation</a>" including midi playback, software samplers, audio effects, routing and mixing; a music logic engine to control the music interactively in real-time; and a synchronization component that makes sure that all players in a session hear the music at exactly the same time, a prerequisite for being able to play together.</p>

<p>To achieve the highest possible level of authenticity, accuracy and audio quality we opted to use the Web Audio API. This case study will discuss some of the challenges we were presented with, and how we solved them. There are already a number of great introductory articles here at HTML5Rocks to get you started with Web Audio, so we’ll jump straight into the deep end of the pool.</p>

<h2 id="toc-topic">Writing custom audio effects</h2>

<p>The Web Audio API has a number of useful effects included in the specification, but we needed  more elaborate effects for our instruments in JAM with Chrome. For example, there is a native delay node in Web Audio, but there are many types of <a href="http://en.wikipedia.org/wiki/Delay_(audio_effect)">delays</a> - stereo delay, ping pong delay, slapback delay and the list goes on. Luckily, all of these are possible to create in Web Audio using the native effect nodes and some imagination.</p>

<p>Since we wanted to be able to use the native nodes and our own custom effects in such a transparent way as possible, we decided that we needed to create a wrapper format that could  achieve this. The native nodes in Web Audio uses its connect method to link nodes together, so we needed to emulate this behaviour. This is what the basic idea looks like:</p>

<pre class="prettyprint">
var MyCustomNode = function(){
    this.input = audioContext.createGain();
    var output = audioContext.createGain();

    this.connect = function(target){
       output.connect(target);
    };
};
</pre>

<p>With this pattern we’re really close to the native nodes. Let’s see how this would be used.</p>

<pre class="prettyprint">
//create a couple of native nodes and our custom node
var gain = audioContext.createGain(),
    customNode = new MyCustomNode(),
    anotherGain = audioContext.createGain();

//connect our custom node to the native nodes and send to the output
gain.connect(customNode.input);
customNode.connect(anotherGain);
anotherGain.connect(audioContext.destination);
</pre>

<img src="/static/images/tutorials/casestudies/jamwithchrome-audio/skeleton.png" alt="Routing the custom node"/>

<p>The only difference between our custom node and a native one is that we have to connect to the custom nodes input property. I’m sure there are ways to circumvent this, but this was close enough for our purposes. This pattern can be further developed to simulate the disconnect methods of native AudioNodes, as well as accommodating user defined inputs/outputs when connecting and so on. Have a look at the <a href="https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html#AudioNode-section">specification</a> to see what the native nodes can do.</p>

<p>Now that we had our basic pattern for creating custom effects, the next step were to actually give the custom node some custom behaviour. Let’s have a look at a slapback delay node.</p>

<h2 id="toc-topic">Slapback like you mean it</h2>

<p>The slapback delay, sometimes called slapback echo, is a classic effect used on a number of instruments, from 50’s style vocals to surf guitars. The effect takes the incoming sound and plays a copy of the sound with a slight delay of approximately 75-250 milliseconds. This gives a feeling of the sound being slapped back, thus the name. We can create the effect like so:</p>

<pre class="prettyprint">
var SlapbackDelayNode = function(){
    //create the nodes we’ll use
    this.input = audioContext.createGain();
    var output = audioContext.createGain(),
        delay = audioContext.createDelay(),
        feedback = audioContext.createGain(),
        wetLevel = audioContext.createGain();

    //set some decent values
    delay.delayTime.value = 0.15; //150 ms delay
    feedback.gain.value = 0.25;
    wetLevel.gain.value = 0.25;

    //set up the routing
    this.input.connect(delay);
    this.input.connect(output);
    delay.connect(feedback);
    delay.connect(wetLevel);
    feedback.connect(delay);
    wetLevel.connect(output);

    this.connect = function(target){
       output.connect(target);
    };
};
</pre>

<img src="/static/images/tutorials/casestudies/jamwithchrome-audio/slapback.png" alt="Internal routing of the slapback node"/>

<p>As some of you might already have realised, this delay could be used with bigger delay times too, and thus become a regular mono delay with feedback. Here's an example using this delay to let you hear what it sounds like.</p> 

<h3 id="toc-topic-subtopic">Mouse down to produce sound</h3>
<div id="dry">Dry</div>
<div id="wet">Wet</div>

<p>There is much that can be done with this simple effect. It’s the base of other effects, like chorus and phasers, but it could also be further developed in its own right by, for example, inserting other effects that only affect the delayed signal.</p>

<h2 id="toc-topic">Routing audio</h2>

<p>When working with different instruments and musical parts in professional audio applications it’s essential to have a flexible routing system that allows you to mix and modulate the sounds in effective ways. In JAM with Chrome we have developed an audio bus system, similar to the ones found in physical mixing boards. This allows us to hook up all instruments that need a reverb effect to a common bus, or channel, and then add the reverb to that bus instead of adding a reverb to each separate instrument. This is a major optimization and it’s quite recommended to do something similar as soon as you start doing more complex applications.</p>

<img src="/static/images/tutorials/casestudies/jamwithchrome-audio/busrouting.png" alt="Routing of the AudioBus"/>

<p>Luckily this is really easy to achieve in Web Audio. We can basically use the skeleton we defined for the effects and use it in the same way.</p>

<pre class="prettyprint">
var AudioBus = function(){
    this.input = audioContext.createGain();
    var output = audioContext.createGain();

    //create effect nodes (Convolver and Equalizer are other custom effects from the library presented at the end of the article)
    var delay = new SlapbackDelayNode(),
        convolver = new tuna.Convolver(),
        equalizer = new tuna.Equalizer();

    //route ‘em
    //equalizer -> delay -> convolver
    this.input.connect(equalizer);
    equalizer.connect(delay.input);
    delay.connect(convolver);
    convolver.connect(output);

    this.connect = function(target){
       output.connect(target);
    };
};
</pre>

<p>This would be used like this:</p>

<pre class="prettyprint">
//create some native oscillators and our custom audio bus
var bus = new AudioBus(),
    instrument1 = audioContext.createOscillator(),
    instrument2 = audioContext.createOscillator(),
    instrument3 = audioContext.createOscillator();

//connect our instruments to the same bus
instrument1.connect(bus.input);
instrument2.connect(bus.input);
instrument3.connect(bus.input);
bus.connect(audioContext.destination);
</pre>

<p>And voila, we’ve applied delay, equalization and reverb (which is a rather expensive effect, performance wise) at half the cost as if we’d applied the effects to each separate instrument. If we wanted to add some extra spice to the bus, we could add two new gain nodes - preGain and postGain - which would allow us to turn off or fade the sounds in a bus in two different ways. The preGain is put before the effects, and postGain is put in the end of the chain. If we then fade the preGain the effects will still resonate after the gain has hit bottom, but if we fade postGain all sound will be muted at the same time.</p>

<h2 id="toc-topic">Where to from here?</h2>

<p>These methods I’ve described here can, and should, be further developed. Things like the input and output of the custom nodes, and the connect methods, could/should be implemented using prototype based inheritance. Buses should be able to create effects dynamically by being passed a list of effects. </p>

<p>To celebrate the release of JAM with Chrome we’ve decided to make our <a href=" https://github.com/Dinahmoe/tuna">framework of effects open source</a>. If this brief introduction has tickled your fancy, please have a look and feel free to contribute. There’s a discussion going on <a href="https://github.com/h5bp/lazyweb-requests/issues/82">here</a> regarding standardizing a format for custom Web Audio entites. Get involved!</p>

<script>
        window.onload = function(){
            
            //make sure we can play the demo
            if(!window.webkitAudioContext){
                document.getElementById("message").innerHTML = "Oops, your browser doesn't support this demo! Please try the latest Google Chrome.";
                return;
            } else{
                if(!window.Audio){
                    document.getElementById("message").innerHTML = "Oops, your browser doesn't support this demo! Please try the latest Google Chrome.";
                    return;
                }
                var a = new Audio(), filename;
                if(!!(a.canPlayType && a.canPlayType('audio/ogg; codecs="vorbis"').replace(/no/, ''))){
                   filename = "/static/audio/guitar_bright_hard_4_2.ogg";
                }
                else if(!!(a.canPlayType && a.canPlayType('audio/mp4; codecs="mp4a.40.2"').replace(/no/, ''))){
				   filename = "/static/audio/guitar_bright_hard_4_2.aac";
                }
                else {
			      document.getElementById("message").innerHTML = "Oops, your browser doesn't support this demo! Please try the latest Google Chrome.";
			      return;
			    }
            }
            
            var audioContext = new AudioContext() || new webkitAudioContext();
            
            //our custom node
            var SlapbackDelayNode = function(){
                //create the nodes we’ll use
                this.input = audioContext.createGain();
                var output = audioContext.createGain(),
                    delay = audioContext.createDelay(),
                    feedback = audioContext.createGain(),
                    wetLevel = audioContext.createGain();

                //set some decent values
                delay.delayTime.value = 0.15; //150 ms delay
                feedback.gain.value = 0.25;
                wetLevel.gain.value = 0.25;

                //set up the routing
                this.input.connect(delay);
                this.input.connect(output);
                delay.connect(feedback);
                delay.connect(wetLevel);
                feedback.connect(delay);
                wetLevel.connect(output);

                this.connect = function(target){
                   output.connect(target);
                };
            };
            
            var dryButton = document.getElementById("dry"),
                wetButton = document.getElementById("wet"),
                buffer,
                bufferSourceDry,
                bufferSourceWet,
                delay = new SlapbackDelayNode();
                
            //route delay
            delay.connect(audioContext.destination);
            
            //event callbacks
            function playDry(){
                bufferSourceDry = audioContext.createBufferSource();
                bufferSourceDry.buffer = buffer;
                bufferSourceDry.connect(audioContext.destination);
                bufferSourceDry.start(audioContext.currentTime);
            }
            function playWet(){
                bufferSourceWet = audioContext.createBufferSource();
                bufferSourceWet.buffer = buffer;
                bufferSourceWet.connect(delay.input);
                bufferSourceWet.start(audioContext.currentTime);
            }
            function stopDry(){
                bufferSourceDry.stop(audioContext.currentTime);
                bufferSourceDry.disconnect();
            }
            function stopWet(){
                bufferSourceWet.stop(audioContext.currentTime);
                bufferSourceWet.disconnect();
            }
            
                
            function setupListeners(){
                dryButton.addEventListener("mousedown", playDry);
                wetButton.addEventListener("mousedown", playWet);
                dryButton.addEventListener("mouseup", stopDry);
                wetButton.addEventListener("mouseup", stopWet);
            }    
                
            //load the guitar sound
            var request = new XMLHttpRequest();
            request.open("GET", filename, true);
            request.responseType = "arraybuffer";
            request.onload = function() {
                audioContext.decodeAudioData(request.response, success, error);
                function success(audioData) {
                    buffer = audioData;
                }
                function error() {
                    console.error("Failed decoding the audio needed. The examples in this article will not work.");
                }
                setupListeners();
            };
            request.send();
        };
    </script>

{% endblock %}
