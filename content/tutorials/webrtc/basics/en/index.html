{% extends "tutorial.html" %}

{% block pagebreadcrumb %}{{ tut.title }}{% endblock %}

{% block html5badge %}
<img src="/static/images/identity/html5-badge-h-multimedia.png" width="133" height="64" alt="This article is powered by HTML5 Audio/Video" title="This article is powered by HTML5 Audio?/Video" />
{% endblock %}

{% block iscompatible %}
return !! (window.RTCPeerConnection || window.webkitDeprecatedPeerConnection || window.webkitRTCPeerConnection);
{% endblock %}

{% block head %}
<style>
.talkinghead:before {
  background-image: url(/static/images/profiles/75/dutton.75.png);
}
</style>
{% endblock %}

{% block content %}

<blockquote>
  WebRTC is a new front in the long war for an open and unencumbered web.
  <cite><a href="http://hacks.mozilla.org/2012/03/video-mobile-and-the-open-web/" title="Brendan Eich blog post: Video, Mobile, and the Open Web">Brendan Eich</a>, inventor of JavaScript</cite>
</blockquote>

<h2 id="toc-disruptive">Real-time communication without plugins</h2>

<p>Imagine a world where your phone, TV and computer could all communicate on a common platform. Imagine it was easy to add video chat and peer-to-peer data sharing to your web application. That's the vision of WebRTC.</p>

<p>Want to try it out? WebRTC is available now in Google Chrome, Opera and Firefox. A good place to start is the simple video chat application at <a href="https://apprtc.appspot.com" title="Simple WebRTC demo" target="_blank">apprtc.appspot.com</a>:</p>

<ol>
  <li>Open <a href="https://apprtc.appspot.com" title="Simple WebRTC demo" target="_blank">apprtc.appspot.com</a> in Chrome, Opera or Firefox.</li>
  <li>Click the Allow button to let the app use your webcam.</li>
  <li>Open the URL displayed at the bottom of the page in a new tab or, better still, on a different computer.</li>
</ol>

 <p>There is a walkthrough of this application <a href="#toc-simple" title="Code walkthrough of apprtc.appspot.com">later in this article</a>.</p>

<h2 id="toc-tldr">Quick start</h2>

<p>Haven't got time to read this article, or just want code?</p>

<ol>
  <li>
    <p>Get an overview of WebRTC from the Google I/O presentation (the slides are <a href="http://io13webrtc.appspot.com" title="Google I/O 2013 WebRTC presentation">here</a>):</p>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/p2HzZkd2A40" frameborder="0" allowfullscreen></iframe>
  </li>
  <li>If you haven't used getUserMedia, take a look at the <a href="http://www.html5rocks.com/en/tutorials/getusermedia/intro/" title="HMTL5 Rocks: Capturing Audio and Video in HTML5" target="_blank">HTML5 Rocks article</a> on the subject, and view the source for the simple example at <a href="http://www.simpl.info/getusermedia" title="Simple getUserMedia example">simpl.info/gum</a>.</li>
  <li>Get to grips with the RTCPeerConnection API by reading through the <a href="#simpleRTCPeerConnectionExample" title="Internal link to simple RTCPeerConnecton example">simple example below</a> and the demo at <a href="http://www.simpl.info/pc" title="WebRTC demo without signaling">simpl.info/pc</a>, which implements WebRTC on a single web page.</li>
  <li>Learn more about how WebRTC uses servers for signaling, and firewall and NAT traversal, by reading through the code and console logs from <a href="https://apprtc.appspot.com" title="Simple WebRTC video chat demo">apprtc.appspot.com</a>.</li>
  <li>Canâ€™t wait and just want to try out WebRTC right now? Try out some of the <a href="https://webrtc.github.io/samples" title="WebRTC Samples">20+ demos</a> that exercise the WebRTC JavaScript APIs.</li>
  <li>Having trouble with your machine and WebRTC? Try out our troubleshooting page <a href="https://test.webrtc.org" title="WebRTC Troubleshooter">test.webrtc.org</a>.</li>
</ol>

<p style="font-weight: bold">Alternatively, jump straight into our <a href="https://www.bitbucket.org/webrtc/codelab" title="WebRTC codelab repository on Bitbucket">WebRTC codelab</a>: a step-by-step guide that explains how to build a complete video chat app, including a simple signaling server.</p>

<h2 id="toc-history">A very short history of WebRTC</h2>

<p>One of the last major challenges for the web is to enable human communication via voice and video: Real Time Communication, RTC for short. RTC should be as natural in a web application as entering text in a text input. Without it, we're limited in our ability to innovate and develop new ways for people to interact.</p>

<p>Historically, RTC has been corporate and complex, requiring expensive audio and video technologies to be licensed or developed in house. Integrating RTC technology with existing content, data and services has been difficult and time consuming, particularly on the web.</p>

<p>Gmail video chat became popular in 2008, and in 2011 Google introduced Hangouts, which use the Google Talk service (as does Gmail). Google bought GIPS, a company which had developed many components required for RTC, such as codecs and echo cancellation techniques. Google open sourced the technologies developed by GIPS and engaged with relevant standards bodies at the IETF and W3C to ensure industry consensus. In May 2011, Ericsson built <a href="https://labs.ericsson.com/developer-community/blog/beyond-html5-peer-peer-conversational-video" title="Beyond HTML5: peer to peer conversational video">the first implementation of WebRTC</a>.</p>

<p>WebRTC has now implemented open standards for real-time, plugin-free video, audio and data communication. The need is real:</p>

  <ul>
    <li>Many web services already use RTC, but need downloads, native apps or plugins. These includes Skype, Facebook (which uses Skype) and Google Hangouts (which use the Google Talk plugin).</li>
    <li>Downloading, installing and updating plugins can be complex, error prone and annoying.</li>
    <li>Plugins can be difficult to deploy, debug, troubleshoot, test and maintain&mdash;and may require licensing and integration with complex, expensive technology. It's often difficult to persuade people to install plugins in the first place!</li>
  </ul>

<p>The guiding principles of the WebRTC project are that its APIs should be open source, free, standardized, built into web browsers and more efficient than existing technologies.</p>

<h2 id="toc-where">Where are we now?</h2>
<p>WebRTC is used in various apps like WhatsApp, Facebook Messenger, appear.in and platforms such as TokBox. There is even an experimental WebRTC enabled iOS Browser named Bowser. WebRTC has also been integrated with <a href="https://labs.ericsson.com/developer-community/blog/beyond-html5-conversational-voice-and-video-implemented-webkit-gtk" title="Ericsson article about WebKitGTK+">WebKitGTK+</a> and <a href="https://www.youtube.com/watch?v=Vm5ebKWKNE8" title="basysKom showcasing WebRTC based video chat in QML application">Qt</a> native apps.</p>

<p>Microsoft added MediaCapture and Stream APIs to <a href="http://blogs.windows.com/msedgedev/2015/05/13/announcing-media-capture-functionality-in-microsoft-edge/" title="media capture functionality in Microsoft Edge">Edge</a>.</p>

<p>WebRTC implements three APIs:</p>
<ul>
  <li><a href="#toc-mediastream" title="Internal link to section for MediaStream (aka getUserMedia)"><code>MediaStream</code></a> (aka <code>getUserMedia</code>)</li>
  <li><code><a href="#toc-rtcpeerconnection" title="Internal link to section for RTCPeerConnection">RTCPeerConnection</a></code></li>
  <li><a href="#toc-rtcdatachannel" title="Internal link to section about RTCDataChannel"><code>RTCDataChannel</code></a></li>
</ul>

<p><code><strong>getUserMedia</strong></code> is available in Chrome, Opera, Firefox and Edge. Take a look at the cross-browser demo at <a href="https://webrtc.github.io/samples/src/content/getusermedia/gum/" title="Simple cross-platform getUserMedia demo">demo</a> and Chris Wilson's <a href="http://webaudiodemos.appspot.com/" title="">amazing examples</a> using <code>getUserMedia</code> as input for Web Audio.</p>

<p><code><strong>RTCPeerConnection</strong></code> is in Chrome (on desktop and for Android), Opera (on desktop and in the latest Android Beta) and in Firefox. A word of explanation about the name: after several iterations, <code>RTCPeerConnection</code> is currently implemented by Chrome and Opera as <code>webkitRTCPeerConnection</code> and by Firefox as <code>mozRTCPeerConnection</code>. Other names and implementations have been deprecated. When the standards process has stabilized, the prefixes will be removed. There's an ultra-simple demo of Chromium's RTCPeerConnection implementation at <a href="https://webrtc.github.io/samples/src/content/peerconnection/pc1/" title="Simple cross-platform peerconnection demo">GitHub</a> and a great video chat application at <a href="https://apprtc.appspot.com" title="Video chat demo">apprtc.appspot.com</a>. This app uses <a href="https://github.com/webrtc/adapter" title="adapter.js JavaScript file">adapter.js</a>, a JavaScript shim, maintained Google with help from the <a href="https://github.com/webrtc/adapter/graphs/contributors" title="WebRTC/adapter contributors">WebRTC community</a>, that abstracts away browser differences and spec changes.</p>

<p><strong><code>RTCDataChannel</code></strong> is supported by Chrome, Opera and Firefox. Check out one of the data channel demos at <a href="https://webrtc.github.io/samples/" title="WebRTC samples">GitHub</a> to see it in action.
</p>

<h3>A word of warning</h3>

<p>Be skeptical of reports that a platform 'supports WebRTC'. Often this actually just means that <code>getUserMedia</code> is supported, but not any of the other RTC components.</p>

<h2 id="toc-first">My first WebRTC</h2>

<p>WebRTC applications need to do several things:</p>

<ul>
  <li>Get streaming audio, video or other data.</li>
  <li>Get network information such as IP addresses and ports, and exchange this with other WebRTC clients (known as <em>peers</em>) to enable connection, even through <a href="http://en.wikipedia.org/wiki/NAT_traversal" title="Wikipedia article: Network Address Translation traversal">NATs</a> and firewalls.</li>
  <li>Coordinate signaling communication to report errors and initiate or close sessions.</li>
  <li>Exchange information about media and client capability, such as resolution and codecs.</li>
  <li>Communicate streaming audio, video or data.</li>
</ul>

<p>To acquire and communicate streaming data, WebRTC implements the following APIs:</p>
<ul>
  <li><a href="https://dvcs.w3.org/hg/audio/raw-file/tip/streams/StreamProcessing.html" title="MediaStream API documentation">MediaStream</a>: get access to data streams, such as from the user's camera and microphone.</li>
  <li><a href="http://dev.w3.org/2011/webrtc/editor/webrtc.html#rtcpeerconnection-interface" title="W3CRTCPeerConnection Editor's draft">RTCPeerConnection</a>: audio or video calling, with facilities for encryption and bandwidth management.</li>
  <li><a href="http://dev.w3.org/2011/webrtc/editor/webrtc.html#rtcdatachannel" title="W3C WebRTC RTCDataChannel Editor's draft">RTCDataChannel</a>: peer-to-peer communication of generic data.</li>
</ul>

<p>(There is detailed discussion of the network and signaling aspects of WebRTC <a href="#signaling" title="Internal link to section about signaling">below</a>.)</p>

<h2 id="toc-mediastream">MediaStream (aka getUserMedia)</h2>

<p>The <a href="http://dev.w3.org/2011/webrtc/editor/getusermedia.html" title="W3C Editor's Draft: Media Capture and Streams">MediaStream API</a> represents synchronized streams of media. For example, a stream taken from camera and microphone input has synchronized video and audio tracks. (Don't confuse MediaStream tracks with the &lt;track&gt; element, which is something <a href="http://www.html5rocks.com/en/tutorials/track/basics/" title="HTML5 Rocks: Getting Started With the Track Element">entirely different</a>.)</p>

<p>Probably the easiest way to understand MediaStream is to look at it in the wild:</p>

<ol>
  <li>In Chrome or Opera, open the demo at <a href="https://webrtc.github.io/samples/src/content/getusermedia/gum/" title="getUserMedia demo">https://webrtc.github.io/samples/src/content/getusermedia/gum</a>.</li>
  <li>Open the console.</li>
  <li>Inspect the <code>stream</code> variable, which is in global scope.</li>
</ol>

<p>Each MediaStream has an input, which might be a MediaStream generated by <code>navigator.getUserMedia()</code>, and an output, which might be passed to a video element or an RTCPeerConnection.</p>

<p>The <code>getUserMedia()</code> method takes three parameters:</p>

<ul>
  <li>A <a href="#toc-constraints" title="Internal link to section about Media Constraints">constraints object</a>.</li>
  <li>A success callback which, if called, is passed a MediaStream.</li>
  <li>A failure callback which, if called, is passed an error object.</li>
</ul>

<p>Each MediaStream has a <code>label</code>, such as'Xk7EuLhsuHKbnjLWkW4yYGNJJ8ONsgwHBvLQ'. An array of MediaStreamTracks is returned by the <code>getAudioTracks()</code> and <code>getVideoTracks()</code> methods.</p>

<p>For the <a href="https://webrtc.github.io/samples/src/content/getusermedia/gum/" title="getUserMedia demo">https://webrtc.github.io/samples/src/content/getusermedia/gum</a>. example, <code>stream.getAudioTracks()</code> returns an empty array (because there's no audio) and, assuming a working webcam is connected, <code>stream.getVideoTracks()</code> returns an array of one MediaStreamTrack representing the stream from the webcam. Each MediaStreamTrack has a kind ('video' or 'audio'), and a label (something like 'FaceTime HD Camera (Built-in)'), and represents one or more channels of either audio or video. In this case, there is only one video track and no audio, but it is easy to imagine use cases where there are more: for example, a chat application that gets streams from the front camera, rear camera, microphone, and a 'screenshared' application.</p>

<p>In Chrome or Opera, the <code>URL.createObjectURL()</code> method converts a MediaStream to a <a href="http://www.html5rocks.com/tutorials/workers/basics/#toc-inlineworkers-bloburis" title="HTML5 Rocks: Blob URLs">Blob URL</a> which can be set as the <code>src</code> of a video element. (In Firefox and Opera, the <code>src</code> of the video can be set from the stream itself.) Since version M25, Chromium-based browsers (Chrome and Opera) allow audio data from <code>getUserMedia</code> to be passed to an audio or video element (but note that by default the media element will be muted in this case).<p>

<p><code>getUserMedia</code> can also be used <a href="http://updates.html5rocks.com/2012/09/Live-Web-Audio-Input-Enabled" title="HTML5 Rocks update from Chris Wilson: live Web Audio input enabled!">as an input node for the Web Audio API</a>:</p>

<pre class="prettyprint">
function gotStream(stream) {
    window.AudioContext = window.AudioContext || window.webkitAudioContext;
    var audioContext = new AudioContext();

    // Create an AudioNode from the stream
    var mediaStreamSource = audioContext.createMediaStreamSource(stream);

    // Connect it to destination to hear yourself
    // or any other node for processing!
    mediaStreamSource.connect(audioContext.destination);
}

navigator.getUserMedia({audio:true}, gotStream);
</pre>

<p>Chromium-based apps and extensions can also incorporate <code>getUserMedia</code>. Adding <code>audioCapture</code> and/or <code>videoCapture</code> <a href="https://developer.chrome.com/extensions/manifest.html#permissions" title="Chrome app/extension permissions documentation">permissions</a> to the manifest enables permission to be requested and granted only once, on installation. Thereafter the user is not asked for permission for camera or microphone access.</p>

<p>Likewise on pages using HTTPS: permission only has to be granted once for for <code>getUserMedia()</code> (in Chrome at least). First time around, an Always Allow button is displayed in the browser's <a href="http://dev.chromium.org/user-experience/infobars" title="Chromium information about the browser infobar">infobar</a>.</p>

<p>Also, Chrome will deprecate HTTP access for getUserMedia() at the end of 2015 due to it being classified as a <a href="https://sites.google.com/a/chromium.org/dev/Home/chromium-security/deprecating-powerful-features-on-insecure-origins" title="Chrome powerful features">Powerful feature</a>. You can already see a warning when invoked on a HTTP page on Chrome M44.</p>

<p>The intention is eventually to enable a MediaStream for any streaming data source, not just a camera or microphone. This would enable streaming from disc, or from arbitrary data sources such as sensors or other inputs.</p>

<p>Note that <code>getUserMedia()</code> must be used on a server, not the local file system, otherwise a <code>PERMISSION_DENIED: 1</code> error will be thrown.</p>

<p><code>getUserMedia()</code> really comes to life in combination with other JavaScript APIs and libraries:</p>

<ul>
   <li><a href="http://webcamtoy.com/app/" title="Webcam Toy site">Webcam Toy</a> is a photobooth app that uses WebGL to add weird and wonderful effects to photos which can be shared or saved locally.</li>
   <li><a href="http://www.shinydemos.com/facekat/" title="FaceKat game">FaceKat</a> is a 'face tracking' game built with <a href="headtrackr library for realtime face and head tracking" title="headtrackr.js ">headtrackr.js</a>.</li>
   <li><a href="http://idevelop.ro/ascii-camera/" title="'ASCII camera' demo">ASCII Camera</a> uses the Canvas API to generate ASCII images.</li>
</ul>

<figure>
  <a href="http://idevelop.ro/ascii-camera/" title="ASCII Camera app"><img src="ascii.png" alt="ASCII image generated by idevelop.ro/ascii-camera" /></a>
  <figcaption>gUM ASCII art!</figcaption>
</figure>

<h3 id="toc-constraints">Constraints</h3>

<p><a href="http://tools.ietf.org/html/draft-alvestrand-constraints-resolution-00#page-4" title="IETF Resolution Constraints draft specification">Constraints</a> have been implemented since Chrome, Firefox and Opera. These can be used to set values for video resolution for <code>getUserMedia()</code> and RTCPeerConnection <code>addStream()</code> calls. The intention is to implement <a href="http://dev.w3.org/2011/webrtc/editor/getusermedia.html#the-model-sources-sinks-constraints-and-states" title="W3C getUserMedia Editor's Draft - The model: sources, sinks, constraints, and states">support for other constraints</a> such as aspect ratio, facing mode (front or back camera), frame rate, height and width, along with an <a href="http://dev.w3.org/2011/webrtc/editor/getusermedia.html#methods-1" title="applyConstraints() API proposal in W3C getUserMedia Editor's Draft"><code>applyConstraints()</code> method</a>. </p>

<p>There's an example at <a href="https://webrtc.github.io/samples/src/content/getusermedia/resolution/l" title="Resolution Constraints example on GitHub">https://webrtc.github.io/samples/src/content/getusermedia/resolution/</a>.</p>

<p>One gotcha: <code>getUserMedia</code> constraints set in one browser tab affect constraints for all tabs opened subsequently. Setting a disallowed value for constraints gives a rather cryptic error message:</p>

<pre class="prettyprint">navigator.getUserMedia error:
NavigatorUserMediaError {code: 1, PERMISSION_DENIED: 1}</pre>

<h4>Screen and tab capture</h4>

<p>Chrome apps also make it possible to share a live 'video' of a single browser tab or the entire desktop via <a href="http://developer.chrome.com/dev/extensions/tabCapture" title="chrome.tabCapture API documentation">chrome.tabCapture</a> and <a href="https://developer.chrome.com/extensions/desktopCapture" title="chrome.desktopCapture API documentation">chrome.desktopCapture</a> APIs. A desktop capture sample extension can be found in the <a href="https://github.com/webrtc/samples/tree/master/src/content/getusermedia/desktopcapture" title="Desktop capture extension demo">WebRTC samples GitHub repository</a>. For screencast, code and more information, see the HTML5 Rocks update: <a href="http://updates.html5rocks.com/2012/12/Screensharing-with-WebRTC" title="HTML5 Rocks update article: Screensharing with WebRTC">Screensharing with WebRTC</a>.

<p>It's also possible to use screen capture as a MediaStream source in Chrome using the experimental chromeMediaSource constraint, as in <a href="https://html5-demos.appspot.com/static/getusermedia/screenshare.html" title="Screenshare demo">this demo</a>. Note that screen capture requires HTTPS and should only be used for development due to it being enabled via a command line flag as explaind in this discuss-webrtc <a href=" https://groups.google.com/forum/#!msg/discuss-webrtc/TPQVKZnsF5g/Hlpy8kqaLnEJ" title="PSA: enable-usermedia-screen-capture will be removed from about://flags"> post</a>.</p>

<h2 id="toc-signaling">Signaling: session control, network and media information</h2>

<p>WebRTC uses RTCPeerConnection to communicate streaming data between browsers (aka peers), but also needs a mechanism to coordinate communication and to send control messages, a process known as signaling. Signaling methods and protocols are <em>not</em> specified by WebRTC: signaling is not part of the RTCPeerConnection API.</p>

<p>Instead, WebRTC app developers can choose whatever messaging protocol they prefer, such as SIP or XMPP, and any appropriate duplex (two-way) communication channel. The <a href="https://apprtc.appspot.com" title="apprtc WebRTC example">apprtc.appspot.com</a> example uses XHR and the Channel API as the signaling mechanism. The <a href="http://www.bitbucket.org/webrtc/codelab" title="WebRTC codelab">codelab</a> we built uses <a href="http://Socket.io" title="Socket.io website">Socket.io</a> running on a <a href="http://nodejs.org/" title="Node website">Node server</a>.</p>

<p>Signaling is used to exchange three types of information:</p>
<ul>
  <li>Session control messages: to initialize or close communication and report errors.</li>
  <li>Network configuration: to the outside world, what's my computer's IP address and port?</li>
  <li>Media capabilities: what codecs and resolutions can be handled by my browser and the browser it wants to communicate with?</li>
</ul>

<p>The exchange of information via signaling must have completed successfully before peer-to-peer streaming can begin.</p>

<p>For example, imagine Alice wants to communicate with Bob. Here's a code sample from the <a href="http://www.w3.org/TR/webrtc/#simple-example" title="WebRTC 1.0: Real-time Communication Between Browsers"> WebRTC W3C Working Draft</a>, which shows the signaling process in action. The code assumes the existence of some signaling mechanism, created in the <code>createSignalingChannel()</code> method. Also note that on Chrome and Opera, RTCPeerConnection is currently prefixed.</p>

<a id="simpleRTCPeerConnectionExample"></a>

<pre class="prettyprint">var signalingChannel = createSignalingChannel();
var pc;
var configuration = ...;

// run start(true) to initiate a call
function start(isCaller) {
    pc = new RTCPeerConnection(configuration);

    // send any ice candidates to the other peer
    pc.onicecandidate = function (evt) {
        signalingChannel.send(JSON.stringify({ "candidate": evt.candidate }));
    };

    // once remote stream arrives, show it in the remote video element
    pc.onaddstream = function (evt) {
        remoteView.src = URL.createObjectURL(evt.stream);
    };

    // get the local stream, show it in the local video element and send it
    navigator.getUserMedia({ "audio": true, "video": true }, function (stream) {
        selfView.src = URL.createObjectURL(stream);
        pc.addStream(stream);

        if (isCaller)
            pc.createOffer(gotDescription);
        else
            pc.createAnswer(pc.remoteDescription, gotDescription);

        function gotDescription(desc) {
            pc.setLocalDescription(desc);
            signalingChannel.send(JSON.stringify({ "sdp": desc }));
        }
    });
}

signalingChannel.onmessage = function (evt) {
    if (!pc)
        start(false);

    var signal = JSON.parse(evt.data);
    if (signal.sdp)
        pc.setRemoteDescription(new RTCSessionDescription(signal.sdp));
    else
        pc.addIceCandidate(new RTCIceCandidate(signal.candidate));
};
</pre>

<p>First up, Alice and Bob exchange network information. (The expression 'finding candidates' refers to the process of finding network interfaces and ports using the <a href="#ice" title="Internal link to more information about the ICE framework">ICE framework</a>.)</p>

<ol>
  <li>Alice creates an RTCPeerConnection object with an <code>onicecandidate</code> handler.</li>
  <li>The handler is run when network candidates become available.</li>
  <li>Alice sends serialized candidate data to Bob, via whatever signaling channel they are using: WebSocket or some other mechanism.</li>
  <li>When Bob gets a candidate message from Alice, he calls <code>addIceCandidate</code>, to add the candidate to the remote peer description.</li>
</ol>

<p>WebRTC clients (known as <strong>peers</strong>, aka Alice and Bob) also need to ascertain and exchange local and remote audio and video media information, such as resolution and codec capabilities. Signaling to exchange media configuration information proceeds by exchanging an <em>offer</em> and an <em>answer</em> using the Session Description Protocol (SDP):</p>

<ol>
  <li>Alice runs the RTCPeerConnection <code>createOffer()</code> method. The callback argument of this is passed an RTCSessionDescription: Alice's local session description.</li>
  <li>In the callback, Alice sets the local description using <code>setLocalDescription()</code> and then sends this session description to Bob via their signaling channel. Note that RTCPeerConnection won't start gathering candidates until <code>setLocalDescription()</code> is called: this is codified in <a href="http://tools.ietf.org/html/draft-ietf-rtcweb-jsep-03#section-4.2.4" title="Javascript Session Establishment Protocol draft-ietf-rtcweb-jsep-03">JSEP IETF draft</a>.</li>
  <li>Bob sets the description Alice sent him as the remote description using <code>setRemoteDescription()</code>.</li>
  <li>Bob runs the RTCPeerConnection <code>createAnswer()</code> method, passing it the remote description he got from Alice, so a local session can be generated that is compatible with hers. The <code>createAnswer()</code> callback is passed an RTCSessionDescription: Bob sets that as the local description and sends it to Alice.</li>
  <li>When Alice gets Bob's session description, she sets that as the remote description with <code>setRemoteDescription</code>.</li>
  <li>Ping!</li>
</ol>

<p>RTCSessionDescription objects are blobs that conform to the <a href="http://en.wikipedia.org/wiki/Session_Description_Protocol" title="Wikipedia article about the Session Description Protocol">Session Description Protocol</a>, SDP. Serialized, an SDP object looks like this:</p>

<pre class="prettyprint">
v=0
o=- 3883943731 1 IN IP4 127.0.0.1
s=
t=0 0
a=group:BUNDLE audio video
m=audio 1 RTP/SAVPF 103 104 0 8 106 105 13 126

// ...

a=ssrc:2223794119 label:H4fjnMzxy3dPIgQ7HxuCTLb4wLLLeRHnFxh810
</pre>

<p>The acquisition and exchange of network and media information can be done simultaneously, but both processes must have completed before audio and video streaming between peers can begin.</p>

<p>The offer/answer architecture described above is called <a href="http://tools.ietf.org/html/draft-ietf-rtcweb-jsep-00" title="IETF JSEP draft proposal">JSEP</a>, JavaScript Session Establishment Protocol. (There's an excellent animation explaining the process of signaling and streaming in <a href="http://www.ericsson.com/research-blog/context-aware-communication/beyond-html5-peer-peer-conversational-video/" title="Ericsson conversational video demo">Ericsson's demo video</a> for its first WebRTC implementation.) </p>

<figure>
  <img src="jsep.png" alt="JSEP architecture diagram" />
  <figcaption>JSEP architecture</figcaption>
</figure>

<p>Once the signaling process has completed successfully, data can be streamed directly peer to peer, between the caller and callee&mdash;or if that fails, via an intermediary relay server (more about that below). Streaming is the job of RTCPeerConnection.</p>

<h2 id="toc-rtcpeerconnection">RTCPeerConnection</h2>

<p>RTCPeerConnection is the WebRTC component that handles stable and efficient communication of streaming data between peers.</p>

<p>Below is a WebRTC architecture diagram showing the role of RTCPeerConnection. As you will notice, the green parts are complex! </p>

<figure>
<a href="http://www.webrtc.org/reference/architecture" title="webrtc.org: architecture diagram"><img src="webrtcArchitecture.png" alt="WebRTC architecture diagram" style="width: 740px; height: 482px;" /></a>
<figcaption>WebRTC architecture (from <a href="http://www.webrtc.org/reference/architecture" title="webrtc.org: architecture diagram">webrtc.org</a>)</figcaption>
</figure>

<p>From a JavaScript perspective, the main thing to understand from this diagram is that RTCPeerConnection shields web developers from the myriad complexities that lurk beneath. The codecs and protocols used by WebRTC do a huge amount of work to make real-time communication possible, even over unreliable networks: </p>

<ul>
  <li>packet loss concealment</li>
  <li>echo cancellation</li>
  <li>bandwidth adaptivity</li>
  <li>dynamic jitter buffering</li>
  <li>automatic gain control</li>
  <li>noise reduction and suppression</li>
  <li>image 'cleaning'.</li>
</ul>

<p>The <a href="#simpleRTCPeerConnectionExample" title="Internal link to W3C RTCPeerConnection example">W3C code above</a> shows a simplified example of WebRTC from a signaling perspective. Below are walkthroughs of two working WebRTC applications: the first is a simple example to demonstrate RTCPeerConnection; the second is a fully operational video chat client.</p>

<h3 id="toc-sans">RTCPeerConnection without servers</h3>

<p>The code below is taken from the 'single page' WebRTC demo at <a href="https://webrtc.github.io/samples/src/content/peerconnection/pc1/" title="WebRTC demo without signaling">https://webrtc.github.io/samples/src/content/peerconnection/pc1</a>, which has local <em>and</em> remote RTCPeerConnection (and local and remote video) on one web page. This doesn't constitute anything very useful&mdash;caller and callee are on the same page&mdash;but it does make the workings of the RTCPeerConnection API a little clearer, since the RTCPeerConnection objects on the page can exchange data and messages directly without having to use intermediary signaling mechanisms.</p>

<p>One gotcha: the optional second 'constraints' parameter of the <code>RTCPeerConnection()</code> constructor is different from the constraints type used by <code>getUserMedia()</code>: see <a href=" http://www.w3.org/TR/webrtc/#constraints" title="W3C Working Draft Constraints section">w3.org/TR/webrtc/#constraints</a> for more information.</p>

<p>In this example, <code>pc1</code> represents the local peer (caller) and <code>pc2</code> represents the remote peer (callee).</p>

<h3>Caller</h3>

<ol>

<li>
<p>Create a new RTCPeerConnection and add the stream from <code>getUserMedia()</code>:</p>
<pre class="prettyprint">
// servers is an optional config file (see TURN and STUN discussion below)
pc1 = new webkitRTCPeerConnection(servers);
// ...
pc1.addStream(localStream); </pre>
</li>

<li>
<p>Create an offer and set it as the local description for <code>pc1</code> and as the remote description for <code>pc2</code>. This can be done directly in the code without using signaling, because both caller and callee are on the same page:</p>
<pre class="prettyprint">
pc1.createOffer(gotDescription1);
//...
function gotDescription1(desc){
  pc1.setLocalDescription(desc);
  trace("Offer from pc1 \n" + desc.sdp);
  pc2.setRemoteDescription(desc);
  pc2.createAnswer(gotDescription2);
}
</pre>
</li>


</ol>

<h3>Callee</h3>

<ol>

<li>
<p>Create <code>pc2</code> and, when the stream from <code>pc1</code> is added, display it in a video element: </p>
<pre class="prettyprint">
pc2 = new webkitRTCPeerConnection(servers);
pc2.onaddstream = gotRemoteStream;
//...
function gotRemoteStream(e){
  vid2.src = URL.createObjectURL(e.stream);
}
</pre>
</li>

</ol>

<h3 id="toc-real">RTCPeerConnection plus servers</h3>

<p>In the real world, WebRTC needs servers, however simple, so the following can happen:</p>

<ul>
  <li>Users discover each other and exchange 'real world' details such as names.</li>
  <li>WebRTC client applications (peers) exchange network information.</li>
  <li>Peers exchange data about media such as video format and resolution.</li>
  <li>WebRTC client applications traverse <a href="http://en.wikipedia.org/wiki/NAT_traversal" title="Wikipedia article: Network Address Translation traversal">NAT gateways</a> and firewalls.</li>
</ul>

<p>In other words, WebRTC needs four types of server-side functionality:</p>
<ul>
  <li>User discovery and communication.</li>
  <li>Signaling.</li>
  <li>NAT/firewall traversal.</li>
  <li>Relay servers in case peer-to-peer communication fails.</li>
</ul>

<a id="stun"></a>
<a id="ice"></a>


<p>NAT traversal, peer-to-peer networking, and the requirements for building a server app for user discovery and signaling, are beyond the scope of this article. Suffice to say that the <a href="http://en.wikipedia.org/wiki/STUN" title="Wikipedia STUN article">STUN</a> protocol and its extension <a href="http://en.wikipedia.org/wiki/Traversal_Using_Relay_NAT" title="Wikipedia article about TURN">TURN</a> are used by the <a href="http://en.wikipedia.org/wiki/Interactive_Connectivity_Establishment" title="Wikipedia article about ICE">ICE</a> framework to enable RTCPeerConnection to cope with NAT traversal and other network vagaries.</p>

<p>ICE is a framework for connecting peers, such as two video chat clients. Initially, ICE tries to connect peers <em>directly</em>, with the lowest possible latency, via UDP. In this process, STUN servers have a single task: to enable a peer behind a NAT to find out its public address and port. (Google has a couple of STUN severs, one of which is used in the apprtc.appspot.com example.)</p>

<figure>
  <img src="stun.png" alt="Finding connection candidates" />
  <figcaption>Finding connection candidates</figcaption>
</figure>

<p>If UDP fails, ICE tries TCP: first HTTP, then HTTPS. If direct connection fails&mdash;in particular, because of enterprise NAT traversal and firewalls&mdash;ICE uses an intermediary (relay) TURN server. In other words, ICE will first use STUN with UDP to directly connect peers and, if that fails, will fall back to a TURN relay server. The expression 'finding candidates' refers to the process of finding network interfaces and ports.</p>

<figure style="margin-bottom: 2em">
  <img src="dataPathways.png" alt="WebRTC data pathways" />
  <figcaption>WebRTC data pathways</figcaption>
</figure>

<p>WebRTC engineer Justin Uberti provides more information about ICE, STUN and TURN in the <a href="https://www.youtube.com/watch?v=p2HzZkd2A40&t=21m12s" title="Google I/O WebRTC presentation: discussion of ICE, STUN and TURN">2013 Google I/O WebRTC presentation</a>. (The presentation <a href="http://io13webrtc.appspot.com/#52" title="Google I/O WebRTC presentation slide: Deploying STUN and TURN">slides</a> give examples of TURN and STUN server implementations.)</p>

<h4 id="toc-simple">A simple video chat client</h4>

<p>The walkthrough below describes the signaling mechanism used by <a href="https://apprtc.appspot.com" title="apprtc.appspot.com video chat application">apprtc.appspot.com</a>.</p>
<blockquote class="talkinghead commentary">If you find this somewhat baffling, you may prefer our <a href="https://www.bitbucket.org/webrtc/codelab" title="WebRTC codelab repository on Bitbucket">WebRTC codelab</a>. This step-by-step guide explains how to build a complete video chat application, including a simple signaling server built with <a href="http://Socket.io" title="Socket.io website">Socket.io</a> running on a <a href="http://nodejs.org/" title="Node website">Node server</a>.</blockquote>

<p>A good place to try out WebRTC, complete with signaling and NAT/firewall traversal using a STUN server, is the video chat demo at <a href="https://apprtc.appspot.com" title="Simple WebRTC demo" target="_blank">apprtc.appspot.com</a>. This app uses <a href="https://github.com/webrtc/adapter" title="adapter.js JavaScript file">adapter.js</a> to cope with different RTCPeerConnection and <code>getUserMedia()</code> implementations.</p>

<p>The code is deliberately verbose in its logging: check the console to understand the order of events. Below we give a detailed walk-through of the code.</p>

<h3>What's going on?</h3>

<p>The demo starts by running the <code>initialize()</code> function:</p>

<pre class="prettyprint">
function initialize() {
    console.log("Initializing; room=99688636.");
    card = document.getElementById("card");
    localVideo = document.getElementById("localVideo");
    miniVideo = document.getElementById("miniVideo");
    remoteVideo = document.getElementById("remoteVideo");
    resetStatus();
    openChannel('AHRlWrqvgCpvbd9B-Gl5vZ2F1BlpwFv0xBUwRgLF/* ...*/');
    doGetUserMedia();
  }
</pre>

<p>Note that values such as the <code>room</code> variable and the token used by <code>openChannel()</code>, are provided by the Google App Engine app itself: take a look at the <a href="https://github.com/webrtc/apprtc/blob/master/src/web_app/html/index_template.html" title="index.html template code in the apprtc repository">index.html template</a> in the repository to see what values are added.</p>

<p>This code initializes variables for the HTML video elements that will display video streams from the local camera (<code>localVideo</code>) and from the camera on the remote client (<code>remoteVideo</code>). <code>resetStatus()</code> simply sets a status message.</p>

<p>The <code>openChannel()</code> function sets up messaging between WebRTC clients:</p>

<pre class="prettyprint">
function openChannel(channelToken) {
  console.log("Opening channel.");
  var channel = new goog.appengine.Channel(channelToken);
  var handler = {
    'onopen': onChannelOpened,
    'onmessage': onChannelMessage,
    'onerror': onChannelError,
    'onclose': onChannelClosed
  };
  socket = channel.open(handler);
}
</pre>

<p>For signaling, this demo uses the Google App Engine <a href="http://code.google.com/appengine/docs/python/channel/overview.html" title="Channel API Overview (Python)" target="_blank">Channel API</a>, which enables messaging between JavaScript clients without polling. (WebRTC signaling is covered in more detail <a href="#toc-signaling" title="WebRTC signaling">above</a>).</p>

<figure>
  <img src="apprtcArchitecture.png" alt="Architecture of the apprtc video chat application" />
  <figcaption>Architecture of the apprtc video chat application</figcaption>
</figure>

<p>Establishing a channel with the Channel API works like this:</p>

<ol>
  <li>Client A generates a unique ID.</li>
  <li>Client A requests a Channel token from the App Engine app, passing its ID.</li>
  <li>App Engine app requests a channel and a token for the client's ID from the Channel API.</li>
  <li>App sends the token to Client A.</li>
  <li>Client A opens a socket and listens on the channel set up on the server.</li>
</ol>

<figure>
  <img src="channelEstablishing.png" alt="The Google Channel API: establishing a channel" />
  <figcaption>The Google Channel API: establishing a channel</figcaption>
</figure>

<p>Sending a message works like this:</p>

<ol>
  <li>Client B makes a POST request to the App Engine app with an update.</li>
  <li>The App Engine app passes a request to the channel.</li>
  <li>The channel carries a message to Client A.</li>
  <li>Client A's onmessage callback is called.</li>
</ol>

<figure>
  <img src="channelSending.png" alt="The Google Channel API: sending a message" />
  <figcaption>The Google Channel API: sending a message</figcaption>
</figure>

<p>Just to reiterate: signaling messages are communicated via whatever mechanism the developer chooses: the signaling mechanism is not specified by WebRTC. The Channel API is used in this demo, but other methods (such as WebSocket) could be used instead.</p>

<p>After the call to <code>openChannel()</code>, the <code>getUserMedia()</code> function called by <code>initialize()</code> checks if the browser supports the <code>getUserMedia</code> API. (Find out more about getUserMedia on <a href="http://www.html5rocks.com/en/tutorials/getusermedia/intro/" title="HMTL5 Rocks: Capturing Audio & Video in HTML5" target="_blank">HTML5 Rocks</a>.) If all is well, onUserMediaSuccess is called:

<pre class="prettyprint">
function onUserMediaSuccess(stream) {
  console.log("User has granted access to local media.");
  // Call the polyfill wrapper to attach the media stream to this element.
  attachMediaStream(localVideo, stream);
  localVideo.style.opacity = 1;
  localStream = stream;
  // Caller creates PeerConnection.
  if (initiator) maybeStart();
}
</pre>

<p>This causes video from the local camera to be displayed in the <code>localVideo</code> element, by creating an <a href="http://www.html5rocks.com/tutorials/workers/basics/#toc-inlineworkers-bloburis" title="HTML5 Rocks: information about Blob URLs">object (Blob) URL</a> for the camera's data stream and then setting that URL as the <code>src</code> for the element. (<code>createObjectURL</code> is used here as a way to get a URI for an 'in memory' binary resource, i.e. the LocalDataStream for the video.) The data stream is also set as the value of <code>localStream</code>, which is subsequently made available to the remote user.</p>

<p>At this point, <code>initiator</code> has been set to 1 (and it stays that way until the caller's session has terminated) so <code>maybeStart()</code> is called:</p>

<pre class="prettyprint">
function maybeStart() {
  if (!started && localStream && channelReady) {
    // ...
    createPeerConnection();
    // ...
    pc.addStream(localStream);
    started = true;
    // Caller initiates offer to peer.
    if (initiator)
      doCall();
  }
}
</pre>

<p>This function uses a handy construct when working with multiple asynchronous callbacks: <code>maybeStart()</code> may be called by any one of several functions, but the code in it is run only when <code>localStream</code> has been defined <em>and</em> <code>channelReady</code> has been set to true <em>and</em> communication hasn't already started. So&mdash;if a connection hasn't already been made, and a local stream is available, and a channel is ready for signaling, a connection is created and passed the local video stream. Once that happens, <code>started</code> is set to true, so a connection won't be started more than once.</p>

<h4 id="toc-RTCPeerConnection-caller">RTCPeerConnection: making a call</h4>

<p><code>createPeerConnection()</code>, called by <code>maybeStart()</code>, is where the real action begins:</p>

<pre class="prettyprint">
function createPeerConnection() {
  var pc_config = {"iceServers": [{"url": "stun:stun.l.google.com:19302"}]};
  try {
    // Create an RTCPeerConnection via the polyfill (adapter.js).
    pc = new RTCPeerConnection(pc_config);
    pc.onicecandidate = onIceCandidate;
    console.log("Created RTCPeerConnnection with config:\n" + "  \"" +
      JSON.stringify(pc_config) + "\".");
  } catch (e) {
    console.log("Failed to create PeerConnection, exception: " + e.message);
    alert("Cannot create RTCPeerConnection object; WebRTC is not supported by this browser.");
      return;
  }

  pc.onconnecting = onSessionConnecting;
  pc.onopen = onSessionOpened;
  pc.onaddstream = onRemoteStreamAdded;
  pc.onremovestream = onRemoteStreamRemoved;
}
</pre>

<p>The underlying purpose is to set up a connection, using a STUN server, with <code>onIceCandidate()</code> as the callback (see <a href="#stun" title="Explanation of what STUN servers do">above</a> for an explanation of ICE, STUN and 'candidate'). Handlers are then set for each of the RTCPeerConnection events: when a session is connecting or open, and when a remote stream is added or removed. In fact, in this example these handlers only log status messages&mdash;except for <code>onRemoteStreamAdded()</code>, which sets the source for the <code>remoteVideo</code> element:</p>

<pre class="prettyprint">
function onRemoteStreamAdded(event) {
  // ...
  miniVideo.src = localVideo.src;
  attachMediaStream(remoteVideo, event.stream);
  remoteStream = event.stream;
  waitForRemoteVideo();
}
</pre>

<p>Once <code>createPeerConnection()</code> has been invoked in <code>maybeStart()</code>, a call is intitiated by creating and offer and sending it to the callee:</p>

<pre class="prettyprint">
function doCall() {
  console.log("Sending offer to peer.");
  pc.createOffer(setLocalAndSendMessage, null, mediaConstraints);
}
</pre>

<p>The offer creation process here is similar to the no-signaling example <a href="#toc-sans" title="RTCPeerConnection sans signaling">above</a> but, in addition, a message is sent to the remote peer, giving a serialized SessionDescription for the offer. This process is handled by <code>setLocalAndSendMessage():</code></p>

<pre class="prettyprint">
function setLocalAndSendMessage(sessionDescription) {
  // Set Opus as the preferred codec in SDP if Opus is present.
  sessionDescription.sdp = preferOpus(sessionDescription.sdp);
  pc.setLocalDescription(sessionDescription);
  sendMessage(sessionDescription);
}
</pre>

<h4 id="toc-signaling-with-channel">Signaling with the Channel API</h4>

<p>The <code>onIceCandidate()</code> callback invoked when the RTCPeerConnection is successfully created in  <code>createPeerConnection()</code> sends information about candidates as they are 'gathered':</p>

<pre class="prettyprint">
function onIceCandidate(event) {
    if (event.candidate) {
      sendMessage({type: 'candidate',
        label: event.candidate.sdpMLineIndex,
        id: event.candidate.sdpMid,
        candidate: event.candidate.candidate});
    } else {
      console.log("End of candidates.");
    }
  }
</pre>

<p>Outbound messaging, from the client to the server, is done by <code>sendMessage()</code> with an XHR request:</p>

<pre class="prettyprint">
function sendMessage(message) {
  var msgString = JSON.stringify(message);
  console.log('C->S: ' + msgString);
  path = '/message?r=99688636' + '&u=92246248';
  var xhr = new XMLHttpRequest();
  xhr.open('POST', path, true);
  xhr.send(msgString);
}
</pre>

<p>XHR works fine for sending signaling messages from the client to the server, but some mechanism is needed for server-to-client messaging: this application uses the Google App Engine Channel API. Messages from the API (i.e. from the App Engine server) are handled by <code>processSignalingMessage()</code>:</p>

<pre class="prettyprint">
function processSignalingMessage(message) {
  var msg = JSON.parse(message);

  if (msg.type === 'offer') {
    // Callee creates PeerConnection
    if (!initiator && !started)
      maybeStart();

    pc.setRemoteDescription(new RTCSessionDescription(msg));
    doAnswer();
  } else if (msg.type === 'answer' && started) {
    pc.setRemoteDescription(new RTCSessionDescription(msg));
  } else if (msg.type === 'candidate' && started) {
    var candidate = new RTCIceCandidate({sdpMLineIndex:msg.label,
                                         candidate:msg.candidate});
    pc.addIceCandidate(candidate);
  } else if (msg.type === 'bye' && started) {
    onRemoteHangup();
  }
}
</pre>

<p>If the message is an answer from a peer (a response to an offer), RTCPeerConnection sets the remote SessionDescription and communication can begin. If the message is an offer (i.e. a message from the callee) RTCPeerConnection sets the remote SessionDescription, sends an answer to the callee, and starts connection by invoking the RTCPeerConnection <code>startIce()</code> method:</p>

<pre class="prettyprint">
function doAnswer() {
  console.log("Sending answer to peer.");
  pc.createAnswer(setLocalAndSendMessage, null, mediaConstraints);
}
</pre>

<p>And that's it! The caller and callee have discovered each other and exchanged information about their capabilities, a call session is initiated, and real-time data communication can begin.</p>

<h3>Network topologies</h3>

<p>WebRTC as currently implemented only supports one-to-one communication, but could be used in more complex network scenarios: for example, with multiple peers each communicating each other directly, peer-to-peer, or via a <a href="http://en.wikipedia.org/wiki/Multipoint_control_unit" title="MCU article on Wikipedia">Multipoint Control Unit</a> (MCU), a server that can handle large numbers of participants and do selective stream forwarding, and mixing or recording of audio and video:</p>

<figure style="margin-bottom: 2em">
  <img src="mcu.png" alt="Multipoint Control Unit topology diagram" />
  <figcaption>Multipoint Control Unit topology example</figcaption>
</figure>



<p>Many existing WebRTC apps only demonstrate communication between web browsers, but gateway servers can enable a WebRTC app running on a browser to interact with devices such as <a href="http://en.wikipedia.org/wiki/Public_switched_telephone_network" title="Wikipedia article about the Public Switched Telephone Network">telephones</a> (aka <a href="https://en.wikipedia.org/wiki/Public_switched_telephone_network" title="Wikipedia: Public Switched Telephone Network">PSTN</a>) and with <a href="http://en.wikipedia.org/wiki/Voice_over_IP" title="Wikipedia article about Voice Over IP">VOIP</a> systems. In May 2012, Doubango Telecom open-sourced the <a href="http://sipml5.org/" title="sipml5 site">sipml5 SIP client</a>, built with WebRTC and WebSocket which (among other potential uses) enables video calls between browsers and apps running on iOS or Android. At Google I/O, Tethr and Tropo demonstrated <a href="http://tethr.tumblr.com/post/25513708436/tethr-and-tropo-in-the-google-i-o-sandbox" title="Tumblr post about Tethr/Tropo demo">a framework for disaster communications</a> 'in a briefcase', using an <a href="http://en.wikipedia.org/wiki/OpenBTS" title="Wikipedia article about OpenBTS">OpenBTS cell</a> to enable communications between feature phones and computers via WebRTC. Telephone communication without a carrier! </p>

<figure>
  <img src="tethr.jpg" alt="Tethr/Tropo demo at Google I/O 2012" />
  <figcaption>Tethr/Tropo: disaster communications in a briefcase</figcaption>
</figure>

<h2 id="toc-rtcdatachannel">RTCDataChannel</h2>

<p>As well as audio and video, WebRTC supports real-time communication for other types of data.</p>

<p>The RTCDataChannel API enables peer-to-peer exchange of arbitrary data, with low latency and high throughput. There's a simple 'single page' demo at <a href="http://webrtc.github.io/samples/src/content/datachannel/datatransfer/" title="RTCDataChannel example">http://webrtc.github.io/samples/src/content/datachannel/datatransfer</a>.</p>

<p>There are many potential use cases for the API, including:</p>
<ul>
  <li>Gaming</li>
  <li>Remote desktop applications</li>
  <li>Real-time text chat</li>
  <li>File transfer</li>
  <li>Decentralized networks</li>
</ul>

<p>The API has several features to make the most of RTCPeerConnection and enable powerful and flexible peer-to-peer communication:</p>
<ul>
  <li>Leveraging of RTCPeerConnection session setup.</li>
  <li>Multiple simultaneous channels, with prioritization.</li>
  <li>Reliable and unreliable delivery semantics.</li>
  <li>Built-in security (DTLS) and congestion control.</li>
  <li>Ability to use with or without audio or video.</li>
</ul>

<p>The syntax is deliberately similar to WebSocket, with a <code>send()</code> method and a <code>message</code> event:</p>

<pre class="prettyprint">
var pc = new webkitRTCPeerConnection(servers,
  {optional: [{RtpDataChannels: true}]});

pc.ondatachannel = function(event) {
  receiveChannel = event.channel;
  receiveChannel.onmessage = function(event){
    document.querySelector("div#receive").innerHTML = event.data;
  };
};

sendChannel = pc.createDataChannel("sendDataChannel", {reliable: false});

document.querySelector("button#send").onclick = function (){
  var data = document.querySelector("textarea#send").value;
  sendChannel.send(data);
};
</pre>

<p>Communication occurs directly between browsers, so RTCDataChannel can be much faster than WebSocket even if a relay (TURN) server is required when 'hole punching' to cope with firewalls and NATs fails.</p>

<p>RTCDataChannel is available in Chrome, Opera and Firefox. The magnificent <a href="http://www.cubeslam.com" title="Cube Slam game">Cube Slam</a> game uses the API to communicate game state: play a friend or play the bear! <a href="http://www.sharefest.me" title="Sharefest file sharing app">Sharefest</a> enables file sharing via RTCDataChannel, and <a href="https://peercdn.com/" title="peerCDN site">peerCDN</a> offers a glimpse of how WebRTC could enable peer-to-peer content distribution.</p>

<p>For more information about RTCDataChannel, take a look at the IETF's <a href="http://tools.ietf.org/html/draft-jesup-rtcweb-data-protocol-00" title="IETF Data Channel draft specification">draft protocol spec</a>.</p>

<h2 id="toc-security">Security</h2>

<p>There are several ways a real-time communication application or plugin might compromise security. For example:</p>
<ul>
  <li>Unencrypted media or data might be intercepted en route between browsers, or between a browser and a server.</li>
  <li>An application might record and distribute video or audio without the user knowing.</li>
  <li>Malware or viruses might be installed alongside an apparently innocuous plugin or application.</li>
</ul>

<p>WebRTC has several features to avoid these problems:</p>

<ul>
  <li>WebRTC implementations use secure protocols such as <a href="http://en.wikipedia.org/wiki/Datagram_Transport_Layer_Security" title="Wikipedia article about Datagram Transport Layer Security">DTLS</a> and <a href="http://en.wikipedia.org/wiki/Secure_Real-time_Transport_Protocol" title="Wikipedia article about Secure Real-time Transport Protocol">SRTP</a>.</li>
  <li>Encryption is mandatory for all WebRTC components, including signaling mechanisms.</li>
  <li>WebRTC is not a plugin: its components run in the browser sandbox and not in a separate process, components do not require separate installation, and are updated whenever the browser is updated.</li>
  <li>Camera and microphone access must be granted explicitly and, when the camera or microphone are running, this is clearly shown by the user interface.</li>
</ul>

<p>A full discussion of security for streaming media is out of scope for this article. For more information, see the <a href="http://www.ietf.org/proceedings/82/slides/rtcweb-13.pdf" title="Slides for IETF Proposed WebRTC Security Architecture">WebRTC Security Architecture</a> proposed by the IETF.</p>


<h2 id="toc-conclusion">In conclusion</h2>

<p>The APIs and standards of WebRTC can democratize and decentralize tools for content creation and communication&mdash;for telephony, gaming, video production, music making, news gathering and many other applications.</p>

<p>Technology doesn't get much more <a href="http://en.wikipedia.org/wiki/Disruptive_innovation" title="Wikipedia article about 'disruptive innovation'">disruptive</a> than this.</p>

<p>We look forward to what JavaScript developers make of WebRTC as it becomes widely implemented. As blogger Phil Edholm <a href="http://www.nojitter.com/post/232901042/webrtc-is-it-a-game-changer" title="nojitter blog post: WebRTC: Is it a Game Changer?">put it</a>, 'Potentially, WebRTC and HTML5 could enable the same transformation for real-time communications that the original browser did for information.'</p>

<h2 id="toc-tools">Developer tools</h2>

<ul>
  <li>WebRTC stats for an ongoing session can be found at:
    <ul>
      <li><strong>chrome://webrtc-internals</strong> page in Chrome</li>
      <li><strong>opera://webrtc-internals</strong> page in Opera</li>
      <li><strong>about:webrtc</strong> page in Firefox</li>
      <li style="margin-bottom: 1.5em;">Example:
        <figure>
          <img src="internals.png" alt="chrome://webrtc-internals page" />
          <figcaption>chrome://webrtc-internals screenshot</figcaption>
        </figure>
      </li>
    </ul>
  </li>
  <li>Cross browser <a href="http://www.webrtc.org/web-apis/interop" title="webrtc.org Firefox/Chrome interop information">interop notes</a></li>
  <li><a href="https://github.com/webrtc/adapter" title="adapter.js JavaScript file">adapter.js</a> is a JavaScript shim for WebRTC, maintained by Google with help from the <a href="https://github.com/webrtc/adapter/graphs/contributors" title="WebRTC/adapter contributors">WebRTC community</a>, that abstracts vendor prefixes, browser differences and spec changes</li>
  <li>To learn more about WebRTC signaling processes, check the <a href="https://apprtc.appspot.com" title="apprtc.appspot.com video chat demo">apprtc.appspot.com</a> log output to the console</li>
  <li>If it's all too much, you may prefer to use a <a href="http://io13webrtc.appspot.com/#69" title="WebRTC frameworks">WebRTC framework</a> or even a complete <a href="http://io13webrtc.appspot.com/#72" title="WebRTC service providers">WebRTC&nbsp;service</a></li>
  <li>Bug reports and feature requests are always appreciated:
    <ul>
      <li><a href="https://code.google.com/p/webrtc/issues/entry" title="Report WebRTC bugs and feature requests">WebRTC bugs</a></li>
      <li><a href="https://www.crbug.com/new" title="Report Chrome bugs and feature requests">Chrome bugs</a></li>
      <li><a href="https://bugs.opera.com/wizard/" title="Report Opera bugs and feature requests">Opera bugs</a></li>
      <li><a href="https://bugzilla.mozilla.org/" title="File a Firefox bug">Firefox bugs</a></li>
      <li><a href="https://github.com/webrtc/samples/issues/new" title="Report WebRTC demo bugs">WebRTC demo bugs</a></li>
      <li><a href="https://github.com/webrtc/adapter/issues/new" title="Report WebRTC demo bugs">Adapter.js bugs</a></li>
    </ul>
  </li>
</ul>

<h2 id="toc-more">Learn more</h2>

<ul>
  <li><a href="https://www.youtube.com/watch?v=p2HzZkd2A40" title="Video of Google I/O WebRTC session, 2013">WebRTC presentation at Google I/O 2013</a> (the slides are at <a href="http://io13webrtc.appspot.com" title="Google I/O 2013 WebRTC presentation">io13webrtc.appspot.com</a>)</li>
  <li><a href="https://www.youtube.com/watch?v=E8C8ouiXHHk" title="Video of Justin Uberti WebRTC session at Google I/O, 27 June 2012">Justin Uberti's WebRTC session at Google I/O 2012</a></li>
  <li>Alan B. Johnston and Daniel C. Burnett maintain a WebRTC book, now in its second edition in print and eBook formats: <a href="http://www.webrtcbook.com" title="WebRTC book information and download">webrtcbook.com</a></li>
  <li><a href="http://www.webrtc.org/" title="webrtc.org">webrtc.org</a> is home to all things WebRTC: demos, documentation and discussion</li>
  <li><a href="https://github.com/webrtc/samples" title="webrtc.org demos">webrtc.org demo page</a>: links to demos</li>
  <li><a href="https://groups.google.com/forum/?fromgroups#!forum/discuss-webrtc" title="discuss-webrt Google Group">discuss-webrtc</a>: Google Group for technical WebRTC discussion</li>
  <li><a href="https://plus.sandbox.google.com/113817074606039822053/posts" title="WebRTC on Google+">+webrtc</a></li>
  <li><a href="https://twitter.com/webrtc" title="WebRTC on Twitter">@webrtc</a></li>
  <li>Google Developers <a href="https://developers.google.com/talk/libjingle/important_concepts#connections" title="Google Developers: Google Talk for Developers">Google Talk documentation</a>, which gives more information about NAT traversal, STUN, relay servers and candidate gathering</li>
  <li><a href="https://github.com/webrtc" title="WebRTC on GitHub">WebRTC on GitHub</a></li>
  <li><a href="http://stackoverflow.com/questions/tagged/webrtc" title="Stack Overflow questions tagged 'webrtc'">Stack Overflow</a> is a good place to look for answers and ask questions about WebRTC</li>
</ul>

<h2 id="toc-standards">Standards and protocols</h2>

<ul>
  <li><a href="http://dev.w3.org/2011/webrtc/editor/webrtc.html" title="W3C Editor's Draft document">The WebRTC W3C Editor's Draft</a></li>
  <li><a href="http://dev.w3.org/2011/webrtc/editor/getusermedia.html" title="W3C Editor's Draft: Media Capture and Streams">W3C Editor's Draft: Media Capture and Streams</a> (aka getUserMedia)</li>
  <li><a href="http://tools.ietf.org/wg/rtcweb/charters" title="IETF Working Group Charter">IETF Working Group Charter</a></li>
  <li><a href="http://tools.ietf.org/html/draft-jesup-rtcweb-data-protocol-01" title="IETF RTCDataChannel documentation">IETF WebRTC Data Channel Protocol Draft</a></li>
  <li><a href="http://tools.ietf.org/html/draft-uberti-rtcweb-jsep-02" title="IETF JSEP documentation">IETF JSEP Draft</a></li>
  <li><a href="http://tools.ietf.org/html/rfc5245" title="IETF proposed standard for ICE">IETF proposed standard for ICE</a></li>
  <li>IETF RTCWEB Working Group Internet-Draft: <a href="http://tools.ietf.org/html/draft-ietf-rtcweb-use-cases-and-requirements-10" title="">Web Real-Time Communication Use-cases and Requirements</a></li>
</ul>

<h2 id="toc-support">WebRTC support summary</h2>

<h3>MediaStream and getUserMedia</h3>
<ul>
  <li>Chrome desktop 18.0.1008+; Chrome for Android 29+</li>
  <li>Opera 18+; Opera for Android 20+</li>
  <li>Opera 12, Opera Mobile 12 (based on the Presto engine)</li>
  <li>Firefox 17+</li>
  <li>Microsoft Edge</li>
</ul>

<h3>RTCPeerConnection</h3>
<ul>
  <li>Chrome desktop 20+ (now 'flagless', i.e. no need to set about:flags); Chrome for Android 29+ (flagless)</li>
  <li>Opera 18+ (on by default); Opera for Android 20+ (on by default)</li>
  <li>Firefox 22+ (on by default)</li>
</ul>

<h3>RTCDataChannel</h3>
<ul>
  <li>Experimental version in Chrome 25, more stable (and with Firefox interoperability) in Chrome 26+; Chrome for Android 29+</li>
  <li>Stable version (and with Firefox interoperability) in Opera 18+; Opera for Android 20+</li>
  <li>Firefox 22+ (on by default)</li>
</ul>

<p>Native APIs for RTCPeerConnection are also available: <a href="http://www.webrtc.org/native-code/native-apis" title="webrtc.org native API documentation">documentation on webrtc.org</a>.</p>

<p>For more detailed information about cross-platform support for APIs such as getUserMedia, see <a href="http://caniuse.com/stream" title="caniuse.com: getUserMedia/Stream support">caniuse.com</a>.</p>

{% endblock %}
